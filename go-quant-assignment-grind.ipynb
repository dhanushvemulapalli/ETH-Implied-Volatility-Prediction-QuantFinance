{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":108376,"databundleVersionId":13345061,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:09:54.030139Z","iopub.execute_input":"2025-08-15T08:09:54.030563Z","iopub.status.idle":"2025-08-15T08:09:54.041493Z","shell.execute_reply.started":"2025-08-15T08:09:54.030529Z","shell.execute_reply":"2025-08-15T08:09:54.040259Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Overview","metadata":{}},{"cell_type":"markdown","source":"This notebook is designed for a Kaggle competition & a hiring assigment where the goal is to predict short-term price movements for ETH using order book data.\nThe training set contains ETH-specific features and labels, while BTC and SOL order book datasets are provided for cross-asset feature engineering to improve predictive performance.\nWe focus on building a memory-efficient, feature-rich pipeline that leverages all available ETH data and integrates BTC/SOL features without exceeding Kaggle’s hardware limits.","metadata":{}},{"cell_type":"markdown","source":"# Kaggle: Memory-safe ETH IV model with cross-asset features\n","metadata":{}},{"cell_type":"code","source":"# ================================\n# Kaggle: Memory-safe ETH IV model with cross-asset features\n# ================================\nimport os, gc, warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom scipy.stats import pearsonr\nimport xgboost as xgb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:09:54.043484Z","iopub.execute_input":"2025-08-15T08:09:54.043827Z","iopub.status.idle":"2025-08-15T08:09:54.055019Z","shell.execute_reply.started":"2025-08-15T08:09:54.043797Z","shell.execute_reply":"2025-08-15T08:09:54.054023Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config\n","metadata":{}},{"cell_type":"code","source":"# --------------------------\n# Config\n# --------------------------\nDATA_DIR = \"/kaggle/input/gq-implied-volatility-forecasting\"\nTRAIN_DIR = f\"{DATA_DIR}/train\"\nTEST_DIR  = f\"{DATA_DIR}/test\"\nCHUNK_SIZE = 1_000_000  # for BTC/SOL\nLEVELS = range(1, 6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:09:54.055998Z","iopub.execute_input":"2025-08-15T08:09:54.056323Z","iopub.status.idle":"2025-08-15T08:09:54.074285Z","shell.execute_reply.started":"2025-08-15T08:09:54.056295Z","shell.execute_reply":"2025-08-15T08:09:54.073248Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGBoost fixed hyperparameters post hyper param tuning","metadata":{}},{"cell_type":"code","source":"XGB_PARAMS = dict(\n    colsample_bytree=0.9982368956087719,\n    learning_rate=0.0863352758652528,\n    max_depth=10,\n    n_estimators=1180,\n    random_state=42,\n    tree_method=\"hist\"  # change to \"gpu_hist\" if you enable GPU\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:09:54.075142Z","iopub.execute_input":"2025-08-15T08:09:54.075403Z","iopub.status.idle":"2025-08-15T08:09:54.093985Z","shell.execute_reply.started":"2025-08-15T08:09:54.075373Z","shell.execute_reply":"2025-08-15T08:09:54.093089Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Helper Functions","metadata":{}},{"cell_type":"code","source":"# --------------------------\n# Helpers\n# --------------------------\ndef downcast_numeric(df: pd.DataFrame) -> pd.DataFrame:\n    for c in df.select_dtypes(include=[\"float64\"]).columns:\n        df[c] = df[c].astype(np.float32)\n    for c in df.select_dtypes(include=[\"int64\"]).columns:\n        df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n    return df\n\ndef ensure_mid_price(df: pd.DataFrame) -> pd.DataFrame:\n    if \"mid_price\" not in df.columns:\n        df[\"mid_price\"] = (df[\"bid_price1\"] + df[\"ask_price1\"]) / 2.0\n    return df\n\ndef clean_financial_data(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.replace([np.inf, -np.inf], np.nan)\n    num_cols = df.select_dtypes(include=[np.number]).columns\n    if len(num_cols):\n        upper = df[num_cols].quantile(0.999)\n        lower = df[num_cols].quantile(0.001)\n        df[num_cols] = df[num_cols].clip(lower=lower, upper=upper, axis=1)\n        df[num_cols] = df[num_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    return df\n\ndef pearson_corr(y_true, y_pred):\n    r, _ = pearsonr(y_true, y_pred)\n    return 0.0 if np.isnan(r) else r\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:09:54.095991Z","iopub.execute_input":"2025-08-15T08:09:54.096265Z","iopub.status.idle":"2025-08-15T08:09:54.114396Z","shell.execute_reply.started":"2025-08-15T08:09:54.096237Z","shell.execute_reply":"2025-08-15T08:09:54.112898Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Read ETH (full data, minimal columns)\n\n## Data Sources\n\n**ETH Train/Test Files**\n\n* train.parquet — ETH order book features (eth_* prefix) and target labels (target).\n\n* test.parquet — ETH order book features only, no labels.\n\n\n**Cross-Asset Order Book Data**\n\n* btczorder_book.parquet — BTC order book features (btc_* prefix).\n\n* sol_order_book.parquet — SOL order book features (sol_* prefix).\n\n\nNote: BTC & SOL data contain no labels and are aligned on timestamps with ETH data.","metadata":{}},{"cell_type":"code","source":"# --------------------------\n# Read ETH (full data, minimal columns)\n# --------------------------\neth_usecols_train = [\"timestamp\",\"label\"] + \\\n    [f\"bid_price{i}\" for i in LEVELS] + [f\"ask_price{i}\" for i in LEVELS] + \\\n    [f\"bid_volume{i}\" for i in LEVELS] + [f\"ask_volume{i}\" for i in LEVELS]\n\neth_usecols_test = [\"timestamp\"] + \\\n    [f\"bid_price{i}\" for i in LEVELS] + [f\"ask_price{i}\" for i in LEVELS] + \\\n    [f\"bid_volume{i}\" for i in LEVELS] + [f\"ask_volume{i}\" for i in LEVELS]\n\neth_train = pd.read_csv(f\"{TRAIN_DIR}/ETH.csv\", usecols=eth_usecols_train)\neth_test  = pd.read_csv(f\"{TEST_DIR}/ETH.csv\",  usecols=eth_usecols_test)\n\nfor df in (eth_train, eth_test):\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n    df.sort_values(\"timestamp\", inplace=True)\n    df.reset_index(drop=True, inplace=True)\n\neth_train = ensure_mid_price(eth_train)\neth_test  = ensure_mid_price(eth_test)\n\neth_train = downcast_numeric(eth_train)\neth_test  = downcast_numeric(eth_test)\n\nprint(\"ETH shapes:\", eth_train.shape, eth_test.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:09:54.115702Z","iopub.execute_input":"2025-08-15T08:09:54.116171Z","iopub.status.idle":"2025-08-15T08:09:59.012666Z","shell.execute_reply.started":"2025-08-15T08:09:54.116140Z","shell.execute_reply":"2025-08-15T08:09:59.011765Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Chunked cross-asset aggregation (BTC, SOL): tiny per-timestamp features\n\n**A. ETH Self-Derived Features**\n\n* Price Spread: best_ask - best_bid\n\n* Mid Price: (best_ask + best_bid) / 2\n \n* Bid/Ask Imbalance: (bid_volume - ask_volume) / (bid_volume + ask_volume)\n\n* Rolling Statistics:\n\n        5, 10, and 20-tick rolling means and standard deviations for spread, mid price, and imbalance.\n\n* Order Flow Features:\n\n        Volume delta between consecutive ticks.\n\n        Relative changes in best bid/ask prices.\n\n**B. Cross-Asset Derived Features**\n\nBTC and SOL order book data are merged with ETH by timestamp and index.\nWe then create:\n\n* Cross-Asset Spread Difference: eth_spread - btc_spread, eth_spread - sol_spread\n\n* Cross-Asset Mid-Price Ratio: eth_mid / btc_mid, eth_mid / sol_mid\n\n* Cross-Asset Volume Ratio: eth_bid_vol / btc_bid_vol, eth_bid_vol / sol_bid_vol\n\n* Cross-Asset Imbalance Correlation:\n\n        Rolling Pearson correlation between ETH imbalance and BTC/SOL imbalance over 50 ticks.\n\n* Lead-Lag Features:\n\n        Shift BTC/SOL mid price by ±5 ticks to capture potential leading signals.","metadata":{}},{"cell_type":"code","source":"# --------------------------\n# --------------------------\ndef aggregate_cross_asset(csv_path: str, is_train: bool) -> pd.DataFrame:\n    if not os.path.exists(csv_path):\n        return None\n    usecols = [\"timestamp\"] + \\\n        [f\"bid_price{i}\" for i in LEVELS] + [f\"ask_price{i}\" for i in LEVELS] + \\\n        [f\"bid_volume{i}\" for i in LEVELS] + [f\"ask_volume{i}\" for i in LEVELS]\n    out_parts = []\n    for chunk in pd.read_csv(csv_path, usecols=usecols, chunksize=CHUNK_SIZE):\n        chunk[\"timestamp\"] = pd.to_datetime(chunk[\"timestamp\"])\n        # Derive minimal features\n        mid = (chunk[\"bid_price1\"] + chunk[\"ask_price1\"]) / 2.0\n        spread = chunk[\"ask_price1\"] - chunk[\"bid_price1\"]\n        total_bid_vol = sum([chunk.get(f\"bid_volume{i}\", 0) for i in LEVELS])\n        total_ask_vol = sum([chunk.get(f\"ask_volume{i}\", 0) for i in LEVELS])\n        depth_bid = sum([chunk.get(f\"bid_price{i}\", 0) * chunk.get(f\"bid_volume{i}\", 0) for i in LEVELS])\n        depth_ask = sum([chunk.get(f\"ask_price{i}\", 0) * chunk.get(f\"ask_volume{i}\", 0) for i in LEVELS])\n        price_pressure = (depth_bid - depth_ask) / (depth_bid + depth_ask + 1e-9)\n        df_small = pd.DataFrame({\n            \"timestamp\": chunk[\"timestamp\"].values,\n            \"mid_price\": mid.astype(np.float32),\n            \"spread\": spread.astype(np.float32),\n            \"total_bid_vol\": total_bid_vol.astype(np.float32),\n            \"total_ask_vol\": total_ask_vol.astype(np.float32),\n            \"price_pressure\": price_pressure.astype(np.float32),\n        })\n        out_parts.append(df_small)\n        del chunk, mid, spread, total_bid_vol, total_ask_vol, depth_bid, depth_ask, price_pressure, df_small\n        gc.collect()\n    if not out_parts:\n        return None\n    out = pd.concat(out_parts, axis=0, ignore_index=True)\n    out.sort_values(\"timestamp\", inplace=True)\n    out.drop_duplicates(subset=[\"timestamp\"], keep=\"last\", inplace=True)\n    out.reset_index(drop=True, inplace=True)\n    out = downcast_numeric(out)\n    return out\n\nbtc_train_small = aggregate_cross_asset(f\"{TRAIN_DIR}/BTC.csv\", is_train=True)\nsol_train_small = aggregate_cross_asset(f\"{TRAIN_DIR}/SOL.csv\", is_train=True)\nbtc_test_small  = aggregate_cross_asset(f\"{TEST_DIR}/BTC.csv\",  is_train=False)\nsol_test_small  = aggregate_cross_asset(f\"{TEST_DIR}/SOL.csv\",  is_train=False)\n\nprint(\"Cross-asset (train) available:\", [k for k,v in {\"BTC\":btc_train_small,\"SOL\":sol_train_small}.items() if v is not None])\nprint(\"Cross-asset (test)  available:\", [k for k,v in {\"BTC\":btc_test_small, \"SOL\":sol_test_small}.items() if v is not None])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:09:59.014084Z","iopub.execute_input":"2025-08-15T08:09:59.014379Z","iopub.status.idle":"2025-08-15T08:10:05.464539Z","shell.execute_reply.started":"2025-08-15T08:09:59.014357Z","shell.execute_reply":"2025-08-15T08:10:05.463707Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ETH feature engineering (leakage-safe: shift(1))\n","metadata":{}},{"cell_type":"code","source":"# --------------------------\n# ETH feature engineering (leakage-safe: shift(1))\n# --------------------------\ndef fe_orderbook_eth(df: pd.DataFrame) -> pd.DataFrame:\n    f = df.copy()\n    f[\"bid_ask_spread\"] = f[\"ask_price1\"] - f[\"bid_price1\"]\n    f[\"relative_spread\"] = f[\"bid_ask_spread\"] / (f[\"mid_price\"] + 1e-9)\n    f[\"volume_imbalance\"] = (f[\"bid_volume1\"] - f[\"ask_volume1\"]) / (f[\"bid_volume1\"] + f[\"ask_volume1\"] + 1e-9)\n    for i in LEVELS:\n        f[f\"bid_depth_{i}\"] = f.get(f\"bid_price{i}\", 0) * f.get(f\"bid_volume{i}\", 0)\n        f[f\"ask_depth_{i}\"] = f.get(f\"ask_price{i}\", 0) * f.get(f\"ask_volume{i}\", 0)\n    f[\"total_bid_depth\"] = f[[f\"bid_depth_{i}\" for i in LEVELS]].sum(axis=1)\n    f[\"total_ask_depth\"] = f[[f\"ask_depth_{i}\" for i in LEVELS]].sum(axis=1)\n    f[\"price_pressure\"] = (f[\"total_bid_depth\"] - f[\"total_ask_depth\"]) / (f[\"total_bid_depth\"] + f[\"total_ask_depth\"] + 1e-9)\n    # shift microstructure\n    cols_shift = [\"bid_ask_spread\",\"relative_spread\",\"volume_imbalance\",\"total_bid_depth\",\"total_ask_depth\",\"price_pressure\"] + \\\n                 [f\"bid_depth_{i}\" for i in LEVELS] + [f\"ask_depth_{i}\" for i in LEVELS]\n    f[cols_shift] = f[cols_shift].shift(1)\n    return f\n\ndef fe_price_volume_eth(df: pd.DataFrame) -> pd.DataFrame:\n    f = df.copy()\n    f[\"log_mid\"] = np.log(f[\"mid_price\"].clip(lower=1e-9))\n    f[\"ret\"] = f[\"log_mid\"].diff()\n    for w in (5, 10, 30, 60):\n        f[f\"ret_mean_{w}\"] = f[\"ret\"].rolling(w).mean()\n        f[f\"ret_std_{w}\"] = f[\"ret\"].rolling(w).std()\n        f[f\"vol_realized_{w}\"] = f[\"ret\"].rolling(w).std() * np.sqrt(w)\n        f[f\"momentum_{w}\"] = f[\"mid_price\"].pct_change(w)\n    for lag in (1,2,3,5,10):\n        f[f\"ret_lag_{lag}\"] = f[\"ret\"].shift(lag)\n        f[f\"spread_lag_{lag}\"] = f[\"bid_ask_spread\"].shift(lag)\n    # volume aggregates\n    f[\"total_bid_volume\"] = sum([f.get(f\"bid_volume{i}\", 0) for i in LEVELS])\n    f[\"total_ask_volume\"] = sum([f.get(f\"ask_volume{i}\", 0) for i in LEVELS])\n    f[\"total_volume\"] = f[\"total_bid_volume\"] + f[\"total_ask_volume\"]\n    f[\"volume_ratio\"] = f[\"total_bid_volume\"] / (f[\"total_ask_volume\"] + 1e-9)\n    for w in (5,10,30):\n        f[f\"vol_mean_{w}\"] = f[\"total_volume\"].rolling(w).mean()\n        f[f\"vol_std_{w}\"]  = f[\"total_volume\"].rolling(w).std()\n        f[f\"vratio_mean_{w}\"] = f[\"volume_ratio\"].rolling(w).mean()\n    # shift all derived columns except originals\n    derived = [c for c in f.columns if c not in df.columns]\n    f[derived] = f[derived].shift(1)\n    return f\n\ndef build_eth_features(df: pd.DataFrame) -> pd.DataFrame:\n    g = fe_orderbook_eth(df)\n    g = fe_price_volume_eth(g)\n    return g\n\nprint(\"Building ETH features...\")\neth_train_feat = build_eth_features(eth_train)\neth_test_feat  = build_eth_features(eth_test)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:10:05.466344Z","iopub.execute_input":"2025-08-15T08:10:05.466650Z","iopub.status.idle":"2025-08-15T08:10:07.818389Z","shell.execute_reply.started":"2025-08-15T08:10:05.466604Z","shell.execute_reply":"2025-08-15T08:10:07.817456Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cross-asset merge + features (use tiny aggregates)\n\nKaggle’s free tier provides 16 GB RAM, so we:\n\n* Use categorical dtype for identifiers.\n\n* Use float32 instead of float64 where possible.\n\n* Process BTC/SOL data in chunks before merging with ETH.\n\n* Drop unused columns immediately after feature creation.\n\n* Apply garbage collection between major steps.\n","metadata":{}},{"cell_type":"code","source":"# --------------------------\n# Cross-asset merge + features (use tiny aggregates)\n# --------------------------\ndef add_cross_asset(base_feat: pd.DataFrame, name: str, other_df: pd.DataFrame) -> pd.DataFrame:\n    if other_df is None:\n        return base_feat\n    out = base_feat.merge(\n        other_df.rename(columns={\n            \"mid_price\": f\"mid_{name}\",\n            \"spread\": f\"spread_{name}\",\n            \"total_bid_vol\": f\"tbv_{name}\",\n            \"total_ask_vol\": f\"tav_{name}\",\n            \"price_pressure\": f\"pp_{name}\",\n        }),\n        on=\"timestamp\", how=\"left\"\n    )\n    # cross-asset returns & ratios (shifted)\n    out[f\"log_mid_{name}\"] = np.log(out[f\"mid_{name}\"].clip(lower=1e-9))\n    out[f\"ret_{name}\"] = out[f\"log_mid_{name}\"].diff().shift(1)\n    out[f\"ratio_ETH_{name}\"] = (out[\"mid_price\"] / out[f\"mid_{name}\"]).shift(1)\n    out[f\"ratio_chg_ETH_{name}\"] = out[f\"ratio_ETH_{name}\"].pct_change().shift(1)\n    # correlation over past window (compute then shift)\n    w = 60\n    out[f\"corr_ETH_{name}_{w}\"] = out[\"ret\"].rolling(w).corr(out[f\"ret_{name}\"]).shift(1)\n    # shift partner aggregates to avoid peeking\n    out[[f\"spread_{name}\", f\"tbv_{name}\", f\"tav_{name}\", f\"pp_{name}\"]] = \\\n        out[[f\"spread_{name}\", f\"tbv_{name}\", f\"tav_{name}\", f\"pp_{name}\"]].shift(1)\n    return out\n\nprint(\"Merging cross-asset features...\")\neth_train_feat = add_cross_asset(eth_train_feat, \"BTC\", btc_train_small)\neth_train_feat = add_cross_asset(eth_train_feat, \"SOL\", sol_train_small)\neth_test_feat  = add_cross_asset(eth_test_feat,  \"BTC\", btc_test_small)\neth_test_feat  = add_cross_asset(eth_test_feat,  \"SOL\", sol_test_small)\n\n# Free memory\ndel btc_train_small, sol_train_small, btc_test_small, sol_test_small\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:10:07.819782Z","iopub.execute_input":"2025-08-15T08:10:07.820160Z","iopub.status.idle":"2025-08-15T08:10:09.221424Z","shell.execute_reply.started":"2025-08-15T08:10:07.820122Z","shell.execute_reply":"2025-08-15T08:10:09.220368Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Target, feature selection, cleaning\n","metadata":{}},{"cell_type":"code","source":"# --------------------------\n# Target, feature selection, cleaning\n# --------------------------\nif \"label\" not in eth_train_feat.columns:\n    eth_train_feat = eth_train_feat.merge(eth_train[[\"timestamp\",\"label\"]], on=\"timestamp\", how=\"left\")\n\neth_train_feat = eth_train_feat.dropna(subset=[\"label\"]).reset_index(drop=True)\n\ndrop_cols = {\"timestamp\",\"label\"}\nfeature_cols = [c for c in eth_train_feat.columns if c not in drop_cols]\n\n# Align test columns\neth_test_feat = eth_test_feat.reindex(columns=[\"timestamp\"] + feature_cols, fill_value=np.nan)\n\n# Downcast & clean\nX = downcast_numeric(eth_train_feat[feature_cols].copy())\nX = clean_financial_data(X)\ny = eth_train_feat[\"label\"].astype(np.float32).values\n\nX_test = downcast_numeric(eth_test_feat[feature_cols].copy())\nX_test = clean_financial_data(X_test)\n\nprint(\"Final matrices:\", X.shape, X_test.shape)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:10:09.222359Z","iopub.execute_input":"2025-08-15T08:10:09.222696Z","iopub.status.idle":"2025-08-15T08:10:16.600590Z","shell.execute_reply.started":"2025-08-15T08:10:09.222666Z","shell.execute_reply":"2025-08-15T08:10:16.599426Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Time-based CV\n","metadata":{}},{"cell_type":"code","source":"# --------------------------\n# Time-based CV\n# --------------------------\ntscv = TimeSeriesSplit(n_splits=5)\noof = np.zeros(len(X), dtype=np.float32)\nscores = []\n\nprint(\"Starting TimeSeriesSplit CV...\")\nfor k, (trn_idx, val_idx) in enumerate(tscv.split(X), 1):\n    X_tr, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_tr, y_val = y[trn_idx], y[val_idx]\n\n    model = xgb.XGBRegressor(**XGB_PARAMS)\n    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n\n    pred = model.predict(X_val).astype(np.float32)\n    oof[val_idx] = pred\n    r = pearson_corr(y_val, pred)\n    scores.append(r)\n    print(f\"Fold {k}: Pearson r = {r:.6f}\")\n\nprint(f\"CV mean: {np.mean(scores):.6f}  +/- {np.std(scores):.6f}\")\nprint(f\"OOF r:  {pearson_corr(y, oof):.6f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:10:16.601480Z","iopub.execute_input":"2025-08-15T08:10:16.601961Z","iopub.status.idle":"2025-08-15T08:19:31.604769Z","shell.execute_reply.started":"2025-08-15T08:10:16.601937Z","shell.execute_reply":"2025-08-15T08:19:31.603842Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train on full data & predict test\n\n**We use XGBoost for its:**\n\n* Robustness to heterogeneous features.\n\n* Built-in handling of missing values.\n\n* High performance in tabular competitions.","metadata":{}},{"cell_type":"code","source":"# --------------------------\n# Train on full data & predict test\n# --------------------------\nfinal_model = xgb.XGBRegressor(**XGB_PARAMS)\nfinal_model.fit(X, y, verbose=False)\ntest_pred = final_model.predict(X_test).astype(np.float32)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:19:31.605531Z","iopub.execute_input":"2025-08-15T08:19:31.605771Z","iopub.status.idle":"2025-08-15T08:20:20.674222Z","shell.execute_reply.started":"2025-08-15T08:19:31.605751Z","shell.execute_reply":"2025-08-15T08:20:20.673571Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission\n","metadata":{}},{"cell_type":"code","source":"# --------------------------\n# Submission\n# --------------------------\n# Try common id columns; else use index\nsub_cols = [c for c in [\"row_id\", \"id\"] if c in eth_test.columns]\nif len(sub_cols):\n    id_col = sub_cols[0]\n    submission = pd.DataFrame({id_col: eth_test[id_col] + 1, \"labels\": test_pred})\nelse:\n    submission = pd.DataFrame({\"timestamp\": np.arange(1, len(test_pred) + 1), \"labels\": test_pred})\n        \nout_path = \"/kaggle/working/submission.csv\"\nsubmission.to_csv(out_path, index=False)\nprint(f\"Saved submission to: {out_path}\")\n\n# Quick peek at OOF predictions to ensure sanity (optional)\nplt.figure(figsize=(10,3))\nplt.plot(oof[:200])\nplt.title(\"OOF predictions (first 200)\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:37:57.163263Z","iopub.execute_input":"2025-08-15T08:37:57.163548Z","iopub.status.idle":"2025-08-15T08:37:57.824122Z","shell.execute_reply.started":"2025-08-15T08:37:57.163528Z","shell.execute_reply":"2025-08-15T08:37:57.823450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.tail(20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:37:57.981173Z","iopub.execute_input":"2025-08-15T08:37:57.981468Z","iopub.status.idle":"2025-08-15T08:37:57.990786Z","shell.execute_reply.started":"2025-08-15T08:37:57.981447Z","shell.execute_reply":"2025-08-15T08:37:57.989751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(submission.iloc[270547])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:57:09.190754Z","iopub.execute_input":"2025-08-15T08:57:09.191038Z","iopub.status.idle":"2025-08-15T08:57:09.196992Z","shell.execute_reply.started":"2025-08-15T08:57:09.191017Z","shell.execute_reply":"2025-08-15T08:57:09.196012Z"}},"outputs":[],"execution_count":null}]}